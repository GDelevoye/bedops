//=========
// Author:  Alex Reynolds & Shane Neph
// Project: starch
// File:    README (test)
//=========



Running Tests
====================================================================================
To run all tests, type:

  $ make all

Names of individual tests refer to specific test categories and are described below.



Overview
====================================================================================
This directory contains the following subdirectories:

. / binaries
  / compression_and_extraction
  / concatenation
  / metadata_integrity
  / original_data
  / timing
  / updating



Binaries
====================================================================================
The "binaries" folder contains platform-specific starch binaries. The archive-v1.2 binaries 
are sourced from our Google Code site. The archive-v1.5 binaries were compiled from a 
private source repository through version edits to starchMetadataHelpers.h. The 
archive-v2.0 binaries are sourced from our Google Code site (generally the latest version
that creates a v2.0 Starch archive -- on May 21 2013 this would be a v2.2 binary release).



Original Data
====================================================================================
The "original_data" folder contains the "original" BED data that goes into testing the 
functionality of starch, unstarch and starchcat. This consists of a 1M-element BED file
created from randomly-selected hg19 chromosomes from data on the UCSC Genome Browser 
MySQL service. This file is roughly 45 Mb in size and should do a decent job in testing 
how we handle buffers and streaming input on a typical workstation.

To create a sorted, random BED input file only, without running tests, run:

  $ make random_bed_file

Results are in "original_data/data/random.bed". See the 
"original_data/src/make_unsorted_random_bed_file.sh" script for more details. 

The remaining folders are described below and represent test categories.



Compression and Extraction
====================================================================================
To ensure backwards compatibility, this tests whether data compressed with (at this 
time, May 21 2013, archive version v1.2, v1.5 and v2.0) starch binaries can each be 
extracted with the two versions of unstarch: the version that created the archive, and 
the current version of unstarch (binary v2.2). Finally, we use 'diff' to ensure that 
the BED input and extracted output are identical.

To run these tests directly:

  $ make compression_and_extraction_tests



Concatenation
====================================================================================
To ensure that starchcat can concatenate Starch archives, we test the following 
use cases:
 
 * joining two or more Starch files of different versions
 * joining two or more Starch files with disjoint chromosome streams
 * joining two or more Starch files with mixed chromosome streams

For the first test, we start with the different versions of archives created in the
compression and extraction test category. The original BED file has 1M elements, so
we take, say, 250K elements from the first archive and recompress them. We take the next
250K elements from the next archive and recompress those. We take the remaining 500K 
elements from the third and final archive and recompress them. If we concatenate these
three smaller archives, we expect that the final archive will contain all 1M original
elements and in the same sorted order. (If there are more, fewer or different elements, 
or if elements are out of order, then this test fails.)

For the second test, we take the random bed file and split it by chromosome into
separate BED files. We then compress these disjoint BED files into Starch archives
and concatenate the disjoint archives into one result. We then diff-test the final 
result with the random bed file.

The third test is similar to the first, except that we work within one version of an
archive. We split the original file into 100K-element subfiles, compress them, and
attempt to concatenate them. We expect that the final archive will have all 1M original
elements in preserved order.

To run these tests directly:

  $ make concatenation_tests



Metadata Integrity
====================================================================================
We extract the JSON-formatted metadata (with --list-json-no-trailing-newline) from a 
v2.0 archive and create a Base64-encoded SHA-1 hash of that data, using OpenSSL, xxd,
and base64 command-line utilities. This is compared with the result stored in the 
archive via --sha1-signature. If the hashes are identical, this provides assurance of
the integrity of the metadata.

Secondly, we review the metadata contents, specifically the per-chromosome element 
count and the overall and unique base counts. We grab a chromosome at random and 
use awk statements to generate "expected" results, which are compared to values 
obtained through per-chromosome <chr> --elements, <chr> --bases and <chr> --bases-uniq 
calls to the Starch archive. We also test all-chromosome results.

To run these tests directly:

  $ make metadata_integrity_tests



Updating
====================================================================================
Starchcat can also update older archives to current archive version, writing new
metadata in the process. We test whether v1.2 and v1.5 archives can be updated to
v2.0 Starch archives. Additionally, we compare extracted BED streams.

To run these tests directly:

  $ make updating_tests



Timing (to be written)
====================================================================================
Here we will provide tests for the amount of time it takes to compress and extract 
archives using the three versions of starch and unstarch. This should give us an idea 
of relative performance improvements (or areas where improvements are needed). The details 
of this test will be available when written.
